{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Baseline\n",
    "\n",
    "Author: Tiago F. Tavares - 2016\n",
    "\n",
    "This document contains a demonstration of an audio classification process. It is based on work by George Tzanetakis [1], with adaptations to comprise good practices in MIR and machine learning research [2]. The discussion begins on how to characterize datasets, proceeds to feature calculation, then to classification processes and, last, to the statistical analysis that allows differentiating elements.\n",
    "\n",
    "## Dataset characterization\n",
    "\n",
    "Audio datasets for classification will be specified using the MIREX format. This format involves .wav files (16 bits/sample, unsigned int format) and an ascii index file that relates each audio file to its label, in the format:\n",
    "\n",
    "    file1 [tab] label1\n",
    "    file2 [tab] label2\n",
    "\n",
    "Articles should report the number of samples contained in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reggae': 100, 'classical': 100, 'country': 100, 'jazz': 100, 'metal': 100, 'pop': 100, 'disco': 100, 'hiphop': 100, 'rock': 100, 'blues': 100}\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def dataset_characterization(dataset_file, dataset_dir=\"\"):\n",
    "    dataset = {} # Dictionary index = filename, content = class\n",
    "    with open(dataset_dir + dataset_file) as f:\n",
    "        for line in f:\n",
    "            p = re.split(\" |,|\\t\", line.rstrip('\\n'))\n",
    "            dataset[p[0]] = p[1]\n",
    "    return dataset\n",
    "\n",
    "def dataset_class_histogram(dataset):\n",
    "    histogram = {}\n",
    "    for data in dataset:\n",
    "        if dataset[data] not in histogram:\n",
    "            histogram[dataset[data]] = 1\n",
    "        else:\n",
    "            histogram[dataset[data]] += 1\n",
    "    return histogram\n",
    "\n",
    "dataset_dir = \"./datasets/gtzan/\"\n",
    "dataset_file = \"labels.txt\"\n",
    "d = dataset_characterization(dataset_file, dataset_dir)\n",
    "print dataset_class_histogram(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "In the feature extraction step audio samples are mapped into a vector space in which dimensions correspond to different features that describe the audio. The main hypothesis behind this system is that perceptually-related sounds are related through spectral low-level features, such that the distance among the vectors that represent the same class is smaller than the distance of vectors representing instances from different classes. Thus, the feature extraction process for each instance is as follows.\n",
    "\n",
    "<!--Thus, audio samples are mapped into a vector space in which the distance between two vectors is small when the corresponding samples sound similar. The acoustic similarity is assumed to be roughly related to low-level features that can be calculated from the sample's spectrogram. Therefore, the feature extraction process for each sample is as follows.-->\n",
    "\n",
    "Initialy, the sample is normalized to zero mean an unit variance, avoiding effects of gain in the processing chain. After that, it is divided into frames of 46.3ms, with an overlap of 50% between subsequent frames. Each frame is multiplied by a Hanning window and has its magnitude spectrum estimated. The magnitude spectra are then used as basis for the calculation of selected features (spectral centroid, spectral roll-off, spectral flatness, spectral flux and 30 mel-frequency cepstral coefficients). The first and second derivatives of each feature are calculated, because they contain information on the audio content variation over time. A sliding window with duration of 40 frames (approximately 1s) is used to estimate the mean and variance of each feature over time. Last, the mean and variance of each mean and variance is calculated. This process estimates a $n$-dimensional vector representation for the audio sample that can be yielded to classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mir3.modules.features as feat\n",
    "import mir3.modules.tool.wav2spectrogram as spec\n",
    "import mir3.modules.features.centroid as cent\n",
    "import mir3.modules.features.rolloff as roll\n",
    "import mir3.modules.features.flatness as flat\n",
    "import mir3.modules.features.flux as flux\n",
    "import mir3.modules.features.mfcc as mfcc\n",
    "import mir3.modules.features.diff as diff\n",
    "import mir3.modules.features.stats as stats\n",
    "reload(stats)\n",
    "import mir3.modules.features.join as join\n",
    "import mir3.modules.tool.to_texture_window as tex\n",
    "\n",
    "\n",
    "def features_gtzan(filename, directory=\"\"):\n",
    "    # Calculate spectrogram (normalizes wavfile)\n",
    "    converter = spec.Wav2Spectrogram()\n",
    "    s = converter.convert(open(directory + filename), window_length=2048, dft_length=2048,\n",
    "                window_step=1024, spectrum_type='magnitude', save_metadata=True)\n",
    "    \n",
    "    # Extract low-level features, derivatives, and run texture windows    \n",
    "    \n",
    "    d = diff.Diff()\n",
    "    features = (cent.Centroid(), roll.Rolloff(), flat.Flatness(), flux.Flux(), mfcc.Mfcc())\n",
    "    \n",
    "    all_feats = None\n",
    "    for f in features:\n",
    "        track = f.calc_track(s) # Feature track\n",
    "        all_feats = join.Join().join([all_feats, track])\n",
    "        dtrack = d.calc_track(track) # Differentiate\n",
    "        all_feats = join.Join().join([all_feats, dtrack])\n",
    "        ddtrack = d.calc_track(dtrack) # Differentiate again\n",
    "        all_feats = join.Join().join([all_feats, ddtrack])    \n",
    "\n",
    "        # Texture window\n",
    "        t = tex.ToTextureWindow().to_texture(all_feats, 40)\n",
    "        \n",
    "    # Statistics\n",
    "    s = stats.Stats()\n",
    "    d = s.stats([t], mean=True, variance=True)    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def low_level_features(dataset_file, dataset_dir=\"\"): # Estimate low-level features. \n",
    "                                                      # Returns sklearn-compatible structures.\n",
    "    d = dataset_characterization(dataset_file, dataset_dir)\n",
    "    labels = []\n",
    "    features = []\n",
    "    progress = FloatProgress(min=0, max=len(d.keys()))\n",
    "    display(progress)\n",
    "    progress.value = 0\n",
    "    for f in d:\n",
    "        feat = features_gtzan(filename=f, directory=dataset_dir)\n",
    "        features.append(feat.data)\n",
    "        labels.append(d[f])\n",
    "        progress.value += 1\n",
    "    \n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reggae': 100, 'classical': 100, 'country': 100, 'jazz': 100, 'metal': 100, 'pop': 100, 'disco': 100, 'hiphop': 100, 'rock': 100, 'blues': 100}\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./datasets/gtzan/\"\n",
    "dataset_file = \"labels.txt\"\n",
    "\n",
    "dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "print dataset_class_histogram(dataset)\n",
    "\n",
    "features, labels = low_level_features(dataset_file, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_from_confusion(c):\n",
    "    ret = []\n",
    "    for i in xrange(c.shape[0]):\n",
    "        n_i = np.sum(c[i,:])\n",
    "        est_i = np.sum(c[:,i])\n",
    "        if n_i > 0:\n",
    "            R = c[i,i] / float(n_i)\n",
    "        else:\n",
    "            R = 0.0\n",
    "        if est_i > 0:\n",
    "            P = c[i,i] / float(est_i)\n",
    "        else:\n",
    "            P = 0.0\n",
    "            \n",
    "        if (R+P) > 0:\n",
    "            F = 2*R*P/(R+P)\n",
    "        else:\n",
    "            F = 0.\n",
    "        ret.append([R, P, F])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The classification process begins by normalizing the feature set so that all features present zero mean and unit variance across all samples. Then, it proceeds to a 10-fold cross validation schema. Each training fold is further split in a 80%/20% ratio into two sets: $t_a$ and $t_b$. These sets are used to optimize the classifier's hyper-parameters in a grid-search that aims to optimize the f1-score in $t_b$. After this, the classifier is tested into the test set for the fold, which allows calculating recall, precision, and f1-score. The function returns the evaluation measures for all folds, averaged over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "\n",
    "def model_comparison(features, labels, models, parameters_to_optimize):\n",
    "    #norm_features = normalize(features)\n",
    "    features = np.array(features)\n",
    "    skf = StratifiedKFold(labels, n_folds=10)\n",
    "\n",
    "\n",
    "    f1 = np.zeros((10,len(models)))\n",
    "    r = np.zeros((10,len(models)))\n",
    "    p = np.zeros((10,len(models)))\n",
    "    progress = FloatProgress(min=0, max=10*len(models))\n",
    "    display(progress)\n",
    "    for m in xrange(len(models)):\n",
    "        n = 0\n",
    "        for train_index, test_index in skf:\n",
    "            X_train, X_test = features[train_index,:], features[test_index,:]\n",
    "            Y_train, Y_test = [labels[i] for i in train_index], [labels[i] for i in test_index]\n",
    "            scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "            X_train = scaler.transform(X_train) \n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2, random_state=0) # 80% train / 20% validation\n",
    "            classifier = GridSearchCV(estimator=copy.deepcopy(models[m]), cv=cv, param_grid=parameters_to_optimize[m], scoring='f1_weighted')\n",
    "            \n",
    "            classifier.fit(X_train, Y_train)\n",
    "            Y_pred = classifier.predict(X_test)\n",
    "            confusion = confusion_matrix(Y_test, Y_pred)\n",
    "            conf = f1_from_confusion(confusion)\n",
    "            conf_all = np.average(conf, axis=0)\n",
    "\n",
    "            r[n,m] = conf_all[0]\n",
    "            p[n,m] = conf_all[1]\n",
    "            f1[n,m] = conf_all[2]\n",
    "            n += 1\n",
    "            \n",
    "            progress.value += 1\n",
    "    return r, p, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_numbers(labels, d):\n",
    "    return [d.keys().index(i) for i in labels]\n",
    "\n",
    "def numbers_to_labels(numbers, d):\n",
    "    return [d.keys[i] for i in numbers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis\n",
    "\n",
    "The results yielded by different classifiers are compared using a t-test (if there are two classifiers) or the ANOVA test (if there are more than two classifiers). This process assumes that each fold of the cross-validation process is an independent measure. P-values lower than 5% indicate a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.62706373  0.75148632]\n",
      "T-test: 3.17668477411e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare as friedman\n",
    "from scipy.stats import wilcoxon as wilcoxon\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import f_oneway as f_oneway\n",
    "\n",
    "gammas = np.logspace(-6, -2, 2)\n",
    "Cs = np.array([1, 1000, 10000])\n",
    "n_neighbors_s = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "params_knn = dict(n_neighbors=n_neighbors_s)\n",
    "params_svm = dict(gamma=gammas, C=Cs)\n",
    "parameters_to_optimize = [params_knn, params_svm]\n",
    "\n",
    "models = [KNeighborsClassifier(), SVC(class_weight='balanced')]\n",
    "\n",
    "\n",
    "recall, precision, f1_score = model_comparison(features, label_to_numbers(labels, dataset_class_histogram(d)), models, parameters_to_optimize)\n",
    "print np.average(f1_score, axis=0)\n",
    "\n",
    "if len(models) > 2:\n",
    "#    print [f1_score[:,i].T for i in range(len(models))]\n",
    "    print \"Anova: \", f_oneway( *f1_score.T  )[1]\n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    print \"T-test:\", ttest( f1_score[:,0].T,  f1_score[:,1].T)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.savetxt(\"features.csv\", features, delimiter=\",\")\n",
    "#print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
